{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -1) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python\n",
    "## modules\n",
    "import time  \n",
    "import sys  \n",
    "import numpy as np \n",
    "import copy\n",
    "## fonctions\n",
    "from datetime                  import datetime\n",
    "from dateutil.relativedelta    import relativedelta\n",
    "\n",
    "# Spark 1.6\n",
    "from pyspark                   import SparkContext \n",
    "from pyspark                   import SparkConf    \n",
    "from pyspark.sql               import SQLContext   \n",
    "from pyspark.sql               import HiveContext  \n",
    "\n",
    "# Spark 2.0\n",
    "from pyspark.sql               import SparkSession \n",
    "\n",
    "# Fonctions\n",
    "from pyspark.sql               import Row\n",
    "from pyspark.sql.types         import *\n",
    "\n",
    "# Machine learning\n",
    "from pyspark.ml                import Pipeline\n",
    "from pyspark.ml.feature        import OneHotEncoder\n",
    "from pyspark.ml.feature        import StringIndexer\n",
    "from pyspark.ml.feature        import VectorIndexer\n",
    "from pyspark.ml.feature        import VectorAssembler\n",
    "from pyspark.ml.evaluation     import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Ancienne librairie de ML\n",
    "from pyspark.mllib.evaluation  import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.3.101:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://steeves:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://steeves:7077 appName=PySparkShell>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.3.101:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://steeves:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7facfd5d65c0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "app_name    = \"Random forest RF\"\n",
    "nb_cores    = 3\n",
    "paralelisme = 3\n",
    "memory      = 3\n",
    "start_load  = time.time()\n",
    "spark_1_6   = False\n",
    "spark_2     = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert( spark_1_6 & spark_2 == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#spark 1.6\n",
    "if spark_1_6:\n",
    "    conf = SparkConf()\n",
    "    conf.setAppName(app_name)\n",
    "    conf.set(\"spark.mesos.coarse\"             , \"True\")\n",
    "    conf.set(\"spark.executor.memory\"          , \"%sg\"%memory)\n",
    "    conf.set(\"spark.driver.memory\"            , \"%sg\"%memory)\n",
    "    conf.set(\"spark.serializer\"               , \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    conf.set(\"spark.kryoserializer.buffer.max\", \"1024m\")\n",
    "    conf.set(\"spark.driver.maxResultSize\"     , \"10g\")\n",
    "    conf.set(\"spark.cores.max\"                , \"%s\"%(nb_cores))\n",
    "    conf.set(\"spark.default.parallelism\"      , \"%s\"%(nb_cores*paralelisme))\n",
    "    conf.set(\"spark.storage.memoryFraction\"   , \"0.5\")\n",
    "    sc         = SparkContext(conf=conf)\n",
    "    sqlContext = HiveContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if spark_2:\n",
    "    spark = SparkSession.builder\\\n",
    "    .config(\"spark.app.name\"                  , app_name                                   )\\\n",
    "    .config(\"spark.cores.max\"                 , \"%s\"%(nb_cores)                            )\\\n",
    "    .config(\"spark.mesos.coarse\"             , \"True\"                                      )\\\n",
    "    .config(\"spark.executor.memory\"          , \"%sg\"%memory                                )\\\n",
    "    .config(\"spark.driver.memory\"            , \"%sg\"%memory                                )\\\n",
    "    .config(\"spark.serializer\"               , \"org.apache.spark.serializer.KryoSerializer\")\\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1024m\"                                     )\\\n",
    "    .config(\"spark.driver.maxResultSize\"     , \"10g\"                                       )\\\n",
    "    .config(\"spark.cores.max\"                , \"%s\"%(nb_cores)                             )\\\n",
    "    .config(\"spark.default.parallelism\"      , \"%s\"%(nb_cores*paralelisme)                 )\\\n",
    "    .config(\"spark.storage.memoryFraction\"   , \"0.5\"                                       )\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.3.101:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://steeves:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7facfd5d65c0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Structure et import du fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_fichier = \"hdfs://steeves:8020/cycliste_cyclistes.csv\"\n",
    "data = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(url_fichier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "colonnes = data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) one hot encoding sur le sexe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import IndexToString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_hot_encoding = StringIndexer(inputCol=\"sexe\", outputCol=\"sexe-num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_one_hot_encoding = one_hot_encoding.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_avec_sexe_en_binaire = model_one_hot_encoding.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cycliste='cycliste_azrc5', sportif=-0.5, age=24, sexe='femme', nb_km=14.9038044082, vitesse=12.0815937384, attente=0.5, sexe-num=0.0),\n",
       " Row(cycliste='cycliste_azrs0', sportif=-0.5, age=67, sexe='femme', nb_km=14.5112928217, vitesse=14.0640955863, attente=0.5, sexe-num=0.0),\n",
       " Row(cycliste='cycliste_azyqs', sportif=6.0, age=27, sexe='femme', nb_km=9.90271533363, vitesse=20.937700539, attente=0.5, sexe-num=0.0),\n",
       " Row(cycliste='cycliste_azqb1', sportif=2.0, age=46, sexe='homme', nb_km=9.73490180628, vitesse=24.0848358156, attente=0.5, sexe-num=1.0),\n",
       " Row(cycliste='cycliste_azulv', sportif=2.0, age=64, sexe='femme', nb_km=5.62040512786, vitesse=11.4958355607, attente=0.5, sexe-num=0.0)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_avec_sexe_en_binaire.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) création d'une colonne \"features\" qui comprend les paramètres explicatifs du sexe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cycliste',\n",
       " 'sportif',\n",
       " 'age',\n",
       " 'sexe',\n",
       " 'nb_km',\n",
       " 'vitesse',\n",
       " 'attente',\n",
       " 'sexe-num']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colonnes = data_avec_sexe_en_binaire.columns\n",
    "colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sportif', 'age', 'nb_km', 'vitesse', 'attente']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colonnes_a_enlever = ['sexe', 'sexe-num', 'cycliste']\n",
    "colonnes_sans_y = [x for x in colonnes if x not in colonnes_a_enlever]\n",
    "colonnes_sans_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vitesse', 'age', 'sportif', 'attente', 'nb_km']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colonnes_a_garder = list(set(colonnes) - set(colonnes_a_enlever))\n",
    "colonnes_a_garder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) création d'un vecteur avec les colonnes à garder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "instance = VectorAssembler(inputCols=colonnes_a_garder, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_avec_col_features = instance.transform(data_avec_sexe_en_binaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cycliste='cycliste_azrc5', sportif=-0.5, age=24, sexe='femme', nb_km=14.9038044082, vitesse=12.0815937384, attente=0.5, sexe-num=0.0, features=DenseVector([12.0816, 24.0, -0.5, 0.5, 14.9038])),\n",
       " Row(cycliste='cycliste_azrs0', sportif=-0.5, age=67, sexe='femme', nb_km=14.5112928217, vitesse=14.0640955863, attente=0.5, sexe-num=0.0, features=DenseVector([14.0641, 67.0, -0.5, 0.5, 14.5113])),\n",
       " Row(cycliste='cycliste_azyqs', sportif=6.0, age=27, sexe='femme', nb_km=9.90271533363, vitesse=20.937700539, attente=0.5, sexe-num=0.0, features=DenseVector([20.9377, 27.0, 6.0, 0.5, 9.9027])),\n",
       " Row(cycliste='cycliste_azqb1', sportif=2.0, age=46, sexe='homme', nb_km=9.73490180628, vitesse=24.0848358156, attente=0.5, sexe-num=1.0, features=DenseVector([24.0848, 46.0, 2.0, 0.5, 9.7349])),\n",
       " Row(cycliste='cycliste_azulv', sportif=2.0, age=64, sexe='femme', nb_km=5.62040512786, vitesse=11.4958355607, attente=0.5, sexe-num=0.0, features=DenseVector([11.4958, 64.0, 2.0, 0.5, 5.6204]))]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_avec_col_features.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) gestion des  colonnes catégorielles / numériques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oneHotEncodeColumns(df, cols):\n",
    "    \"\"\"\n",
    "    Convertit une colonne contenant n modalité \n",
    "    en n colonne ne comprenant qu'une seule valeur.\n",
    "    (Supprime les effets d'ordre des valeurs numériques)\n",
    "    Parameters:\n",
    "        df : matrice à modifier \n",
    "            dataframe\n",
    "        cols : noms des colonnes à indexer\n",
    "            list de chaine de caractère\n",
    "            \n",
    "    Return: dataframe\n",
    "    \"\"\"\n",
    "    from pyspark.ml.feature import OneHotEncoder\n",
    "    newdf = df\n",
    "    for col in cols:\n",
    "        onehotenc = OneHotEncoder(inputCol=col, outputCol=col+\"-onehot\", dropLast=False)\n",
    "        newdf     = onehotenc.transform(newdf).drop(col)\n",
    "        newdf     = newdf.withColumnRenamed(col+\"-onehot\", col)\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1) catégories => numériques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "typeString = [x[0] for x in data.dtypes if x[1]=='string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "typeString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colString_without_Y = copy.copy(typeString)\n",
    "colString_without_Y.remove(colY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2 = indexStringColumns(data, typeString)\n",
    "data2.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2) numériques => one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "typeDouble = [x[0] for x in data2.dtypes if x[1]=='double']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data3 = oneHotEncodeColumns(data2, colString_without_Y)\n",
    "data3.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3) plusieurs colonnes => un vecteur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choix des colonnes\n",
    "features = data3.columns\n",
    "features.remove(colY)\n",
    "# Assembleur\n",
    "assemblor = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "# Application\n",
    "data4 = assemblor.transform(data3)\n",
    "data4.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    label = [colY]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4) projection, et cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data5 = data4.select(\"features\", colY)\n",
    "data5.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data5.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data5.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data6 = data5.dropDuplicates()\n",
    "data6.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5) équilibrage des classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data6.groupBy(colY).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_examples = 10000\n",
    "income_0             = data6.filter(\"income == 0\")\n",
    "_10000_income_0      = income_0.sample(False, nb_examples/float(income_0.count()))\n",
    "\n",
    "income_1             = data6.filter(\"income == 1\")\n",
    "_10000_income_1      = income_1.sample(False, nb_examples/float(income_1.count()))\n",
    "\n",
    "classes_equilibrees  = _10000_income_0.union(_10000_income_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print _10000_income_0.count(), _10000_income_1.count(), classes_equilibrees.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Division en jeu de test, et jeu d'apprentissage\n",
    "(trainingData, testData) = classes_equilibrees.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1) variation du nombre d'arbres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_max_arbre = 20\n",
    "for ntree in range(1,nb_max_arbre) :\n",
    "    rf          = RandomForestClassifier(labelCol=colY, numTrees=ntree)\n",
    "    model       = rf.fit(trainingData)\n",
    "    predictions = model.transform(testData)\n",
    "\n",
    "    evaluator = MulticlassClassificationEvaluator(  labelCol      = \"income\" , \n",
    "                                                    predictionCol = \"prediction\"   , \n",
    "                                                    metricName    = \"accuracy\"     )\n",
    "    accuracy  = evaluator.evaluate(predictions)\n",
    "    error     = 1 - accuracy\n",
    "\n",
    "    print(\"%s arbre => Accuracy = %g, Error = %s\" % (ntree, accuracy, error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluator.isLargerBetter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2) variation de la profondeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_forets = [1, 10, 20]\n",
    "test_depth  = [5, 10, 20]\n",
    "for ntree in test_forets:\n",
    "    for depth in test_depth:\n",
    "        rf          = RandomForestClassifier(labelCol=colY, numTrees=ntree, maxDepth=depth)\n",
    "        model       = rf.fit(trainingData)\n",
    "        predictions = model.transform(testData)\n",
    "\n",
    "        evaluator = MulticlassClassificationEvaluator(  labelCol      = \"income\" , \n",
    "                                                        predictionCol = \"prediction\"   , \n",
    "                                                        metricName    = \"accuracy\"     )\n",
    "        accuracy  = 1 - evaluator.evaluate(predictions)\n",
    "\n",
    "        print(\"%s arbre, depth = %s => Error = %g\" % (ntree, depth, accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time as now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3) Etendue de la forêt\n",
    "(on peut aller prendre un café)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_forets = [ 20, 30,  50]\n",
    "test_depth  = [ 30] # limite à 30 de profondeurs \n",
    "for ntree in test_forets:\n",
    "    for depth in test_depth:\n",
    "        debut       = now()\n",
    "        # modélisation :\n",
    "        rf          = RandomForestClassifier(labelCol=colY, numTrees=ntree, maxDepth=depth,)\n",
    "        model       = rf.fit(trainingData)\n",
    "        predictions = model.transform(testData)\n",
    "        # mesure de la performance :\n",
    "        evaluator   = MulticlassClassificationEvaluator(  labelCol      = \"income\" , \n",
    "                                                          predictionCol = \"prediction\"   , \n",
    "                                                          metricName    = \"accuracy\"     )\n",
    "        accuracy     = 1 - evaluator.evaluate(predictions)\n",
    "        duree        = now()-debut\n",
    "        print(\"{0} arbre, depth = {1} => Error = {2:3.4}, duree = {3:5} sec \".format (ntree, depth, accuracy, duree))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tentative de récupération des features importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "    newdf       = copy.copy(data)\n",
    "    inputCol_1  = newdf.columns[1]\n",
    "    outputCol_1 = col+\"-num\"\n",
    "    indexer     = StringIndexer(inputCol=inputCol_1, outputCol=outputCol_1)\n",
    "    model       = indexer.fit(newdf)\n",
    "    newdf_1     = model.transform(newdf)\n",
    "    newdf_1.select(inputCol_1, outputCol_1).dropDuplicates().show()\n",
    "\n",
    "    inputCol_2   = outputCol_1\n",
    "    outputCol_2  = col+\"-onehot\"\n",
    "    onehotenc    = OneHotEncoder(inputCol=inputCol_2, outputCol=outputCol_2, dropLast=False)\n",
    "    newdf_2      = onehotenc.transform(newdf_1)\n",
    "    newdf_2.select(inputCol_1, outputCol_1, outputCol_2).dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) changement des classifieurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "classifiers = { \"NaiveBayes\"         : NaiveBayes(labelCol=colY)         , \n",
    "                \"GBTClassifier\"      : GBTClassifier(labelCol=colY)      , \n",
    "                \"LogisticRegression\" : LogisticRegression(labelCol=colY) }\n",
    "best_accuracy   = 0\n",
    "best_classifier = \"\"\n",
    "for classifierName,classifier in classifiers.iteritems():\n",
    "    debut       = now()\n",
    "    model       = classifier.fit(trainingData)\n",
    "    predictions = model.transform(testData)\n",
    "\n",
    "    evaluator   = MulticlassClassificationEvaluator(  labelCol      = \"income\" , \n",
    "                                                      predictionCol = \"prediction\"   , \n",
    "                                                      metricName    = \"accuracy\"     )\n",
    "    accuracy    = evaluator.evaluate(predictions)\n",
    "    error       = 1 - accuracy\n",
    "    duree       = now() - debut\n",
    "    print(\"{0:20} => Accuracy = {1:4.3}, Error = {2:4.3}, duree = {3:5.3} sec\".format (classifierName ,  \n",
    "                                                                                       accuracy       , \n",
    "                                                                                       error          , \n",
    "                                                                                       duree         ))\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy   = accuracy\n",
    "        best_classifier = classifierName\n",
    "        \n",
    "print \"best_classifier = %s, best_accuracy = %s\"%(best_classifier, best_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
