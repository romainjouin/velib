{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "# Spark:\n",
    "from pyspark     import SparkConf\n",
    "from pyspark     import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Python :\n",
    "from datetime import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) configuration de spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "il y aura 3 executors avec chacun 2 coeurs (possiblement sur moins de machines)\n"
     ]
    }
   ],
   "source": [
    "user           = \"romain - Analyse cycliste\"\n",
    "ip_fares       = \"54.37.12.49\"\n",
    "master         = \"spark://54.37.12.49:7077\"\n",
    "executor_cores = 2 # Ne prendra que des machines qui ont des exécuteurs qui peuvent avoir ce nombre de coeurs\n",
    "                   # nb de cores par executor (doit être inférieur au nombre de CPU par machine)\n",
    "nb_cores_max   = 6 # détermine le nombre d'executeur max => max( arrondi.min(nb_cores_max/executor_cores)\n",
    "parallelism    = 4 * executor_cores\n",
    "nb_executor    = nb_cores_max//executor_cores\n",
    "current_date   = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "appName        = \"%s le %s\"%(user, current_date)\n",
    "print(\"il y aura %s executors avec chacun %s coeurs (possiblement sur moins de machines)\"%(nb_executor, executor_cores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) création d'un objet \"configuration\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dico_conf = {\n",
    "    \"spark.app.name\"             : appName        ,\n",
    "    \"spark.master\"               : master         ,\n",
    "    \"spark.cores.max\"            : nb_cores_max   , \n",
    "    \"spark.executor.cores\"       : executor_cores ,\n",
    "    \"spark.default.parallelism\"  : parallelism    , \n",
    "    \"spark.python.worker.memory\" : \"1042M\"        ,\n",
    "}\n",
    "conf = SparkConf()\n",
    "for k,v in dico_conf.items():\n",
    "    conf = conf.set(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4) création d'un spark context et d'une spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'app-20180227043024-0389'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc    = SparkContext.getOrCreate(conf=conf)\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "sc.applicationId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) créons une dataframe : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path = \"hdfs://54.37.12.49:8020/cyclistes/cycliste_cyclistes.csv\"\n",
    "df        = spark.read.option(\"header\", \"true\").\\\n",
    "                    option(\"inferSchema\", \"true\").\\\n",
    "                    csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) changer les strings en int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "string_Indexer = StringIndexer()\n",
    "string_Indexer.setInputCol(\"sexe\")\n",
    "string_Indexer.setOutputCol(\"label\")\n",
    "\n",
    "string_indexer_model = string_Indexer.fit(df)\n",
    "cyclistes_doubles    = string_indexer_model.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) création d'une colonne de vecteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cycliste', 'sportif', 'age', 'sexe', 'nb_km', 'vitesse', 'attente', 'label']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colonnes             = cyclistes_doubles.columns\n",
    "colonnes_a_supprimer = [\"sexe\", \"sexe-num\", \"cycliste\", \"label\"]\n",
    "colonnes_sans_y      = [col for col in colonnes if col not in colonnes_a_supprimer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_y = VectorAssembler()\n",
    "x_y.setInputCols(colonnes_sans_y)\n",
    "x_y.setOutputCol(\"features\")\n",
    "points = x_y.transform(cyclistes_doubles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sportif', 'age', 'nb_km', 'vitesse', 'attente']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colonnes_sans_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cycliste',\n",
       " 'sportif',\n",
       " 'age',\n",
       " 'sexe',\n",
       " 'nb_km',\n",
       " 'vitesse',\n",
       " 'attente',\n",
       " 'label',\n",
       " 'features']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 10) création d'un jeu de test et d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splits              = points.randomSplit([0.8, 0.2])\n",
    "jeu_d_apprentissage = splits[0].cache()\n",
    "jeu_de_test         = splits[1].cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11) création d'un algo (cart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt          = DecisionTreeClassifier()\n",
    "dtmodel     = dt.fit(jeu_d_apprentissage)\n",
    "predictions = dtmodel.transform(jeu_de_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cycliste='cycliste_aze10', sportif=-0.5, age=37, sexe='femme', nb_km=11.050541331, vitesse=19.2038127355, attente=0.5, label=1.0, features=DenseVector([-0.5, 37.0, 11.0505, 19.2038, 0.5]))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeu_d_apprentissage.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1) mesure de qualité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7582058616942723"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "bceval = BinaryClassificationEvaluator()\n",
    "bceval.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12) enregistrement et loading sur hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_file = \"hdfs://54.37.12.49:8020/cart_model_for_cycliste_v5\"\n",
    "dtmodel.save(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassificationModel\n",
    "loaded_model = DecisionTreeClassificationModel.load(output_file)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
