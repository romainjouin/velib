{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemple de compter le nombre d'occurence de mots de plus de trois lettres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) on vérifie la connection au cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f777c2cbda0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f777c13e710>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) on lit un fichier dans une rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = sc.textFile(\"/home/spark/spark/README.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) on définit une fonction à appliquer sur chaque ligne\n",
    "=> Ici on splitte sur les mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splitter= lambda ligne: ligne.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mots = file.flatMap(lambda line : splitter(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) on va filtrer sur les mots qui ont plus de trois caractère\n",
    "=> choix arbitraire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mots_de_trois_caracteres = mots.filter(lambda mot : len(mot)>3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) on créé une rdd en mode clef valeur en retournant un tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clef_valeur              = mots_de_trois_caracteres.map(lambda mot : (mot,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) on récupère le résultat au niveau du driver, et on réduit avec l'opérateur add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import add "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "denombrement_long_mots   = clef_valeur.reduceByKey(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) ici on est à la fin du DAG, et on exécute une action pour l'activer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(denombrement_long_mots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1) action de récupérer des valeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apache', 1),\n",
       " ('Spark', 16),\n",
       " ('provides', 1),\n",
       " ('high-level', 1),\n",
       " ('APIs', 1),\n",
       " ('Scala,', 1),\n",
       " ('Java,', 1),\n",
       " ('optimized', 1),\n",
       " ('engine', 1),\n",
       " ('supports', 2)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denombrement_long_mots.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2) action de compter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mots.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3) action de trier par valeur décroissante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spark', 16),\n",
       " ('using', 5),\n",
       " ('Please', 4),\n",
       " ('with', 4),\n",
       " ('also', 4),\n",
       " ('build', 4),\n",
       " ('including', 4),\n",
       " ('documentation', 3),\n",
       " ('example', 3),\n",
       " ('Hadoop', 3)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denombrement_long_mots.sortBy(lambda x: x[1], ascending=False).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4) action de supprimer des lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spark', 16),\n",
       " ('using', 5),\n",
       " ('Please', 4),\n",
       " ('also', 4),\n",
       " ('including', 4),\n",
       " ('build', 4),\n",
       " ('with', 4),\n",
       " ('documentation', 3),\n",
       " ('general', 3),\n",
       " ('example', 3)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mots_a_eliminier = [\"using\", \"Please\"]\n",
    "denombrement_long_mots.filter(lambda x: x not in mots_a_eliminier).sortBy(lambda x: x[1], ascending=False).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.5) action de lister les mots par longueur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# l'astuce consiste à retourner une liste comme valeur\n",
    "inversion_clef_valeur = denombrement_long_mots.map(lambda k_v : (k_v[1], [k_v[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(16, ['Spark']),\n",
       " (2,\n",
       "  ['supports',\n",
       "   'Interactive',\n",
       "   'Shell',\n",
       "   'following',\n",
       "   '1000:',\n",
       "   'Python',\n",
       "   'programs',\n",
       "   './bin/run-example',\n",
       "   'SparkPi',\n",
       "   'examples',\n",
       "   'locally',\n",
       "   'class',\n",
       "   'tests',\n",
       "   'guidance',\n",
       "   'Hadoop,',\n",
       "   'refer',\n",
       "   'particular',\n",
       "   'Hive',\n",
       "   'This',\n",
       "   '`examples`',\n",
       "   'that',\n",
       "   'cluster',\n",
       "   'detailed',\n",
       "   'building',\n",
       "   'Python,',\n",
       "   'Scala',\n",
       "   'shell:',\n",
       "   'command,',\n",
       "   'which',\n",
       "   'should',\n",
       "   'return']),\n",
       " (4, ['Please', 'with', 'also', 'build', 'including']),\n",
       " (1,\n",
       "  ['<class>',\n",
       "   'will',\n",
       "   'locally.',\n",
       "   'MASTER',\n",
       "   'environment',\n",
       "   'running',\n",
       "   'submit',\n",
       "   'cluster.',\n",
       "   'mesos://',\n",
       "   '\"yarn\"',\n",
       "   'thread,',\n",
       "   '\"local[N]\"',\n",
       "   'threads.',\n",
       "   'MASTER=spark://host:7077',\n",
       "   'Many',\n",
       "   'given.',\n",
       "   'Running',\n",
       "   'Tests',\n",
       "   'first',\n",
       "   'requires',\n",
       "   '[building',\n",
       "   '[run',\n",
       "   'tests](http://spark.apache.org/developer-tools.html#individual-tests).',\n",
       "   'Versions',\n",
       "   'core',\n",
       "   'talk',\n",
       "   'protocols',\n",
       "   'same',\n",
       "   'your',\n",
       "   'runs.',\n",
       "   '[\"Specifying',\n",
       "   'Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)',\n",
       "   'Configuration',\n",
       "   'review',\n",
       "   '[Contribution',\n",
       "   'guide](http://spark.apache.org/contributing.html)',\n",
       "   'information',\n",
       "   'fast',\n",
       "   'computing',\n",
       "   'system',\n",
       "   'Data.',\n",
       "   'graphs',\n",
       "   'data',\n",
       "   'rich',\n",
       "   'higher-level',\n",
       "   'DataFrames,',\n",
       "   'Streaming',\n",
       "   'stream',\n",
       "   'processing.',\n",
       "   '<http://spark.apache.org/>',\n",
       "   'Online',\n",
       "   'find',\n",
       "   'documentation,',\n",
       "   'page](http://spark.apache.org/documentation.html).',\n",
       "   'file',\n",
       "   'contains',\n",
       "   'setup',\n",
       "   'built',\n",
       "   'Maven](http://maven.apache.org/).',\n",
       "   'programs,',\n",
       "   'build/mvn',\n",
       "   '-DskipTests',\n",
       "   'clean',\n",
       "   'package',\n",
       "   '(You',\n",
       "   'need',\n",
       "   'pre-built',\n",
       "   'package.)',\n",
       "   'thread',\n",
       "   'option',\n",
       "   'Maven,',\n",
       "   '[\"Parallel',\n",
       "   'builds',\n",
       "   'More',\n",
       "   'available',\n",
       "   'from',\n",
       "   '[\"Building',\n",
       "   'info',\n",
       "   '[http://spark.apache.org/developer-tools.html](the',\n",
       "   'Tools',\n",
       "   'page).',\n",
       "   'easiest',\n",
       "   'through',\n",
       "   './bin/spark-shell',\n",
       "   'sc.parallelize(1',\n",
       "   'prefer',\n",
       "   './bin/pyspark',\n",
       "   'sc.parallelize(range(1000)).count()',\n",
       "   'Programs',\n",
       "   'comes',\n",
       "   'sample',\n",
       "   'directory.',\n",
       "   'Apache',\n",
       "   'provides',\n",
       "   'high-level',\n",
       "   'APIs',\n",
       "   'Scala,',\n",
       "   'Java,',\n",
       "   'optimized',\n",
       "   'engine',\n",
       "   'computation',\n",
       "   'analysis.',\n",
       "   'tools',\n",
       "   'MLlib',\n",
       "   'machine',\n",
       "   'learning,',\n",
       "   'GraphX',\n",
       "   'graph',\n",
       "   'processing,',\n",
       "   'Documentation',\n",
       "   'latest',\n",
       "   'programming',\n",
       "   'guide,',\n",
       "   '[project',\n",
       "   'README',\n",
       "   'only',\n",
       "   'basic',\n",
       "   'instructions.',\n",
       "   'Building',\n",
       "   '[Apache',\n",
       "   'run:',\n",
       "   'this',\n",
       "   'downloaded',\n",
       "   'more',\n",
       "   'than',\n",
       "   'Maven',\n",
       "   '3\"](https://cwiki.apache.org/confluence/display/MAVEN/Parallel+builds+in+Maven+3).',\n",
       "   'project',\n",
       "   'site,',\n",
       "   'Spark\"](http://spark.apache.org/docs/latest/building-spark.html).',\n",
       "   'development',\n",
       "   'tips,',\n",
       "   'developing',\n",
       "   'IDE,',\n",
       "   'Useful',\n",
       "   'Developer',\n",
       "   'start',\n",
       "   'scala>',\n",
       "   '1000).count()',\n",
       "   'Alternatively,',\n",
       "   'Example',\n",
       "   'several',\n",
       "   'them,',\n",
       "   '`./bin/run-example',\n",
       "   '[params]`.',\n",
       "   'example:',\n",
       "   'variable',\n",
       "   'when',\n",
       "   'spark://',\n",
       "   'URL,',\n",
       "   'YARN,',\n",
       "   '\"local\"',\n",
       "   'abbreviated',\n",
       "   'name',\n",
       "   'package.',\n",
       "   'instance:',\n",
       "   'print',\n",
       "   'usage',\n",
       "   'help',\n",
       "   'params',\n",
       "   'Testing',\n",
       "   'Spark](#building-spark).',\n",
       "   'Once',\n",
       "   'built,',\n",
       "   'using:',\n",
       "   './dev/run-tests',\n",
       "   'module,',\n",
       "   'individual',\n",
       "   'Note',\n",
       "   'About',\n",
       "   'uses',\n",
       "   'library',\n",
       "   'HDFS',\n",
       "   'other',\n",
       "   'Hadoop-supported',\n",
       "   'storage',\n",
       "   'systems.',\n",
       "   'Because',\n",
       "   'have',\n",
       "   'changed',\n",
       "   'different',\n",
       "   'versions',\n",
       "   'must',\n",
       "   'against',\n",
       "   'version',\n",
       "   'distribution',\n",
       "   'Thriftserver',\n",
       "   'distributions.',\n",
       "   '[Configuration',\n",
       "   'Guide](http://spark.apache.org/docs/latest/configuration.html)',\n",
       "   'online',\n",
       "   'overview',\n",
       "   'configure',\n",
       "   'Spark.',\n",
       "   'Contributing',\n",
       "   'started',\n",
       "   'contributing',\n",
       "   'project.']),\n",
       " (3, ['example', 'Hadoop', 'general', 'documentation']),\n",
       " (5, ['using'])]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ainsi on peut concaténer les listes\n",
    "inversion_clef_valeur.reduceByKey(lambda x,y: x+y).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
